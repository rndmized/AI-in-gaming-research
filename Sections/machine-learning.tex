\section{Machine Learning}
Even though Machine Learning has been a field of stud in computer science for the last 30 years, it was not until very recently that game developers started to implement it in their games. The lack of enthusiasm for adaptable behaviour in games typically responds to the fact that it is not really that important for a game to "learn". However, more and more developers release their games as services rather than products. Replayablity has become a huge factor, if using the same strategy over and over achieves always victory it gets tedious, more a chore than a challenge. A game that can learn adapt to its players has a higher chance to provide its users with more enjoyable situations.

\subsection{Defining Behaviour with AI}
Most games nowadays, either single or multi-player, have NPCs. Whether they are enemies, allies, unnamed or story characters it is important to define a behaviour pattern for therm to perform. However, players tend to reject erratic or inconsistent behaviour from agents, breaking game immersion. To create realistic behaviours that act in a more "human-like" manner is the aim of many AI researchers in the video game industry. In some cases, the goal of the agent is not to perform an action as perfect as a computer would, being the most efficient, but rather try to simulate imperfection. As well, in order for NPC to act as other players is necessary to emulate the lack of information a real player might have such as not being able to detect an enemy that is out of sight but act alert to the possibility of him coming. With that in mind Marvin T. Chan et al [12] designed an AI that emulated driving like a human would do through a race track providing a more challenging experience for the player. As stated in the paper "Although the playerâ€™s goal within the game is to win the race against the game-controlled car, the AI techniques adopted in the game are primarily designed to give the player an enjoyable time racing his or her car. In other words, the objective of winning by either side is not given the highest priority."
 


\vspace{2mm}
Defining the behaviour of the AI requires the expertise to be able to analyse and describe accurately such behaviour and script it accordingly. Chek Tien Tan and Ho-lun Cheng[11] proposed an agent personality representation model to provide an adaptable agent that can perform across a variety of games of different genres exhibiting a plausible adapted behaviour. The Tactical Agent Personality (TAP) is a framework model of progressive learning, to test its capabilities and evaluate the adaptability of the model three scenarios were created. In each scenario, TAP had a series of predefined behaviour with randomly assigned weights to start with and after every iteration a process starter to assimilate new weights (greedy search) and to assign a random value to seek new paths.

\vspace{2mm}
In the first scenario the model was tested against a First Person Shooter (FPS) environment. Three sets of experiments were performed. In the first set the AI was determined at random. In the second set the AI adapted its behaviour based on player performance. In the third set the AI adapted its behaviour base on the NPCs performance. The output of 500 tests per set demonstrated that the highest level of adaptability occurred when AI adaptation was based on the NPCs behaviour rather than player performance.

\vspace{2mm}
In the second scenario the model was tested against a Real-Time Strategy (RTS) environment. In addition to TAP, another layer of decision making was added, the Strategic Agent Personality (SAP). While TAP defined NPC behaviour at an individual level (short-term decisions), SAP determined their actions as a group (long-term decisions).

\vspace{2mm}
In the third scenario the model was tested against a Role-Playing Game (RPG) environment. In this test, the model was modified to switch the weight from the nodes or actions to the edges representing the temporary transitions between actions, Temporary Tactical Agent Personality (TTAP). The game consisted in two groups of characters 1 player and 5 NPCs. The first group implemented TAP while the second implemented TTAP. After running the experiment 500 times, the TTAP demonstrated a quicker adaptation but at the 500 the difference between the two models grow shorter and draws became more frequent.

\vspace{2mm}
TAP presents some interesting points such as high versatility and adaptation that reduces the need of specific scripting for actions escaping from more traditional approaches like Finite State Machines. TAP and SAP present a good performance and the scalability has low impact, however TTAP does not represent a better model over time and presents scalability issues as calculating the weight on the edges increments the overhead by a significant amount.

\subsection{Perfect versus Imperfect Knowledge}
As previously mentioned, in modern player-bot engagement-based video
games, bot responses and behaviour are designed to be
as realistic as possible. Is because of that, bots are designed to detect players just within its visibility area and not further than that. That is called Imperfect Knowledge. In games such as checkers or chess, both player and computer have complete visibility of the area (board) and all information about what is in it, thus we call this Perfect Knowledge. To provide the player with a sense of fairness it is necessary to program the bot to behave as it has imperfect knowledge. To do so several approaches make use of the principle of "neighbourhood"

\vspace{2mm}
In their article, Peter K. K. Loh and Edmond C. Prakash, evaluate the performance of the existing Moving Target Search Algorithms through simulation:
\begin{itemize}
	\item Basic Moving Target Search (BMTS)
	\item Weighted Moving Target Search (WMTS)
	\item Commitment and Deliberation Moving Target Search (CDMTS)
\end{itemize}
\vspace{2mm}
After simulation the results are compared against the Abstraction Moving Target Search Algorithm (AMTS) design, proposed in their paper. When compared, AMTS outperforms the three MST algorithms, with higher exploration moves, but the lowest MTS move. However further investigation is required as scalability might become an issue. Even though it has downsides, the algorithm presents a interesting approach in the matter.

\vspace{2mm}
Another approach includes Machine Learning to learn and generate strategies to increase performance while maintaining imperfect knowledge. Beaulac and Larribe [13] proposed a model based on Hidden Markov Models to narrow down the possible locations of a hidden mobile agent and pair it with machine learning to create a heat map where "hotter" areas are more likely to be traversed by such agent and design strategies around that. To test the model, the experiments presents a simple game (pirate-themed), where the player has to reach two different points in the map. There are "parrots" that indicate to the AI if the mobile agent is in its vicinity, providing more information but still maintaining the imperfect knowledge premise. The model demonstrated its effectiveness but the model requires further adjustments. 