<html><!-- #BeginTemplate "/Templates/basic.dwt" -->
<head>
<!-- #BeginEditable "doctitle" --> 
<title>Game Studies 0101: Selmer Bringsjord: Is it possible to build Dramatically Compelling Interactive Digital Entertainment?</title>
<!-- #EndEditable --> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta NAME="description" CONTENT="The first scholarly journal on computer games">
<meta NAME="Keywords" CONTENT="computer games, game, play, videogames, digital culture, cybertext, 
cyberculture">
<meta NAME="copyright" CONTENT="GameStudies and each author">
<style type="text/css">

<!--

a {  text-decoration: none;; color: #000000;}

A:hover

{

color: #000000; text-decoration: underline}

h1 {  font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 120%; margin-bottom: 6px; font-style: normal; font-weight: bold; margin-top: 6px; padding-top: 0px; padding-bottom: 0px; margin-right: 4px; margin-left: 4px}

h2 {  font-family: Verdana, Arial, Helvetica, sans-serif; margin-bottom: 0px; font-size: 115%; font-style: normal; font-weight: bold; margin-right: 4px; margin-left: 4px}

h3 {  font-family: Verdana, Arial, Helvetica, sans-serif; font-weight: normal; margin-bottom: 0px; font-size: 110%; margin-right: 4px; margin-left: 4px; margin-top: 6px}

p {  font-family: Verdana, Arial, Helvetica, sans-serif; margin-top: 0px; margin-bottom: 6px; font-size: 70%; margin-right: 4px; margin-left: 4px; line-height: 160%}

h4 {  font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 65%; font-style: normal; font-weight: normal; margin-top: 4px; margin-bottom: 4px; margin-right: 32px; margin-left: 32px}

h5 {  font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 80%; font-style: normal; font-weight: bold; margin-top: 0px; margin-right: 4px; margin-bottom: 0px; margin-left: 4px}

h6 { font-family: Verdana, Arial, Helvetica, sans-serif; margin-top: 0px; margin-bottom: 6px; font-size: 70%; margin-right: 12px; margin-left: 4px ; font-weight: normal}
li {  font-family: Arial, Helvetica, sans-serif; font-size: 100%}
dt {  font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 11px; line-height: 18px; margin-left: 4px}
caption {  font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 11px}

-->

</style>

</head>

<body bgcolor="#FFFFFF" topmargin=0 leftmargin=0>
<table   border="0" cellspacing="0" cellpadding="0">
  <tr align="left" valign="bottom"> 
    <td width="56" height="65">&nbsp;</td>
    <td width="156" height="65" colspan="2"> <a href="/"><img src="../../gfx/logo_left.gif" width="126" height="40" border="0"></a> 
    </td>
    <td width="234" height="65" colspan="3"> <a href="/"><img src="../../gfx/logo_right.gif" width="187" height="40" border="0"></a> 
    </td>
    <td width="156" height="65" colspan="2"> <font size="1" face="Verdana, Arial, Helvetica, sans-serif">the 
      international journal of computer game research</font></td>
    <td width="143" height="65"><!-- #BeginEditable "issue" --><!-- #BeginLibraryItem "/Library/0101topright.lbi" -->
<br>
<a href="http://www.gamestudies.org/0101/"><font face="Verdana, Arial, Helvetica, sans-serif" size="1">volume 
1, issue 1<br>
July 2001</font></a><!-- #EndLibraryItem --><!-- #EndEditable --></td>
  </tr>
  <tr align="left" > 
    <td height="1" colspan="9"></td>
  </tr>
  <tr align="left"  bgcolor="#C11083"> 
    <td  height="6" colspan="9"><img src="../../gfx/1x1.gif" width="1" height="1"></td>
  </tr>
  <tr align="left" valign="middle"> 
    <td width="56" height="21"><font size="1" face="Verdana, Arial, Helvetica, sans-serif"><img src="../../gfx/icon_none.gif" width="1" height="21"></font></td>
    <td width="78" height="21"> <font size="1" face="Verdana, Arial, Helvetica, sans-serif"><a href="/">home</a></font></td>
    <td width="78" height="21">&nbsp; </td>
    <td width="78" height="21"><font size="1" face="Verdana, Arial, Helvetica, sans-serif"><a href="../about.html">about</a></font> 
    </td>
    <td width="78" height="21">&nbsp; </td>
    <td width="78" height="21"><font size="1" face="Verdana, Arial, Helvetica, sans-serif"><a href="http://gamestudies.org/0601/archive">archive</a></font></td>
    <td width="78" height="21">&nbsp;</td>
    <td width="98" height="21">&nbsp;</td>
    <td width="143" height="21"> <!-- #BeginEditable "icons" --><!-- #EndEditable -->&nbsp; 
      <img src="../../gfx/icon_none.gif" width="1" height="21"> </td>
  </tr>
  <tr align="left"> 
    <td  height="1" colspan="9" bgcolor="#000000"><img src="../../gfx/1x1.gif" width="1" height="1"></td>
  </tr>
  <tr align="center" valign="top"> 
    <td width="39" height="18" align="left" >&nbsp;</td>
    <td align="left" width="156" height="*" colspan="2">&nbsp;<!-- #BeginEditable "left" -->
      <h6><b>Selmer Bringsjord</b> specializes in the logico-mathematical and 
        philosophical foundations of Artificial Intelligence and Cognitive Science. 
      </h6>
      <h6>He received a bachelor's degree from the University of Pennsylvania, 
        and a PhD from Brown University.</h6>
      <h6> Since 87 he has been on faculty in the Departments of Philosophy, Psychology 
        and Cognitive Science, and Computer Science, at Rensselaer Polytechnic 
        Institute (RPI) in Troy, New York.</h6>
      <h6> Bringsjord was on Rensselaer's team that won the prestigious Hesburgh 
        Award (1995) for excellence in undergraduate education (for technology-based 
        intereactive learning). He was also a Lilly Fellow in 1989. He is co-director 
        of the Autopoesis Project (in AI and creativity).</h6>
      <h6> Bringsjord is author of the critically acclaimed What Robots Can & 
        Can't Be (1992, Kluwer; ISBN 0-7923-1662-2), which is concerned with the 
        future of attempts to create robots that behave as humans. Two new technical 
        books, Superminds: A Defense of Uncomputable Cognition, and Artificial 
        Intelligence and Literary Creativity: Inside the Mind of Brutus, A Storytelling 
        Machine, are out this year (Kluwer Academic/Lawrence Erlbaum). His book 
        Abortion: A Dialogue has been published this past Fall by Hackett. </h6>
      <h6>He has lectured and interviewed in person across the United States, 
        and abroad. </h6>
      <!-- #EndEditable -->&nbsp; 
    </td>
    <td height="*" width="410" colspan="5" align="left"> <!-- #BeginEditable "main" --> 
      <H1 align=left><B><a name="top"></a>Is It Possible to Build Dramatically 
        Compelling Interactive Digital Entertainment</B> <BR>
        (in the form, e.g., of computer games)?<A 
name=tex2html1 href="#presented"><sup>*</sup></a><A 
name=tex2html1 href="#foot13"><SUP>1</SUP></A> <BR>
      </H1>
      <p><i>by </i>Selmer Bringsjord</p>
      <p><i>Rensselaer Polytechnic Institute (RPI)</i></p>
      <p>&nbsp;</p>
      <P align=left></P>
      <P> 
      <p>Lots of computer games are compelling. E.g., I find even current computerized 
        poker games quite compelling, and I find <EM>The Sims</EM> downright fascinating; 
        doubtless you have your own favorites. But our planet isn't graced by 
        even one <EM>dramatically compelling</EM> computer game (or, more generally, 
        one such interactive digital entertainment). The movie <EM>T2</EM>, Dante's 
        <EM>Inferno</EM>, <EM>Hamlet</EM>, Gibson's prophetic <EM>Neuromancer</EM>, 
        the plays of Ibsen -- these things are dramatically compelling: they succeed 
        in no small part because they offer captivating narrative, and all that 
        that entails (e.g., engaging characters). There is no analogue in the 
        interactive digital arena, alas. Massively multi-player online games are 
        digital, interactive, and entertaining -- but they have zero literary 
        power (which explains why, though <EM>T2</EM> engages young kids through 
        at least middle-aged professors, such games are demographically one-dimensional). 
        The same can be said, by my lights, for all other electronic genres. </p>
      <DIV> 
        <P>This state of affairs won't change unless a number of key challenges 
          are conquered; and conquering them will require some seminal advances 
          in the intersection of artificial intelligence (AI) and narrative. (E.g., 
          since interactive digital narrative will need to be crafted and massaged 
          <EM>as the story is unfolding</EM>, computers, not slow-by-comparison 
          humans, will need to be enlisted as at least decent dramatists -- but 
          getting a computer to be a dramatist requires remarkable AI.) In this 
          paper, I discuss one of these challenges for the start of the new millennium: 
          the problem of building dramatically compelling virtual characters. 
          Within this challenge I focus upon one property such characters presumably 
          must have: viz., autonomy. </P>
      </DIV>
      <P> 
      <P> </p>
      <h1><a name=SECTION00010000000000000000>The Issue</a> </h1>
      <p>We have dramatically compelling <em>non</em>-interactive digital (= electronic) 
        entertainment: sit down and watch <em>The Matrix</em> (see Figure <a 
href="#ais.show.emotion.matrix">1</a>) or <em>T2</em> (see Figure 
        <a 
href="#terminator.pov">2</a>). We have compelling interactive 
        digital entertainment; my favorites include console-based sports games.<a name=tex2html3 
href="index.html#foot34"><sup>2</sup></a> We have dramatically compelling 
        interactive entertainment; for example, improvizational theatre. What 
        we <em>don't</em> have is dramatically compelling interactive digital 
        entertainment. People sometimes tell me that there is a counter-example 
        to my negative view to be found in online multi-player games, but though 
        such games are digital, interactive, and entertaining -- they have zero 
        literary power (which explains why, though <em>T2</em> engages young kids 
        through at least middle-aged professors, such games are demographically 
        one-dimensional). The same can be said, by my lights, for all other electronic 
        genres. Can we build systems that imply and affirmative answer to the 
        title of this paper? My answer is: ``Maybe, maybe not; but at any rate 
        I can tell you, at least in broad strokes, what some of the hurdles are, 
        from the standpoint of AI. And I can tell you, today, about one specific 
        hurdle." 
      <p><br>
      <div align=center><a name=ais.show.emotion.matrix>&nbsp;</a><a 
name=291>&nbsp;</a> 
        <table width="50%">
          <caption><strong>Figure 1:</strong> From a Dramatically Compelling Scene 
          in <em>The Matrix</em></caption>
          <tbody> 
          <tr> 
            <td><!-- MATH: $\includegraphics[width=3in]{matrix2.ps}$ --><img 
      height=259 alt=\includegraphics[width=3in]{matrix2.ps} 
      src="2_files/img3.gif" width=345 align=bottom 
border=0></td>
          </tr>
          </tbody> 
        </table>
      </div>
      <p>&nbsp;</p>
      <h1><a name=SECTION00020000000000000000>Realism About Narrative and AI</a> 
      </h1>
      <p>Before we go any further, let's make sure we're realistic about the driving 
        question, and thereby start with a provisional answer of ``I don't know." 
        Such realism will buck the trend. For unfortunately, realistic positions 
        on the advance of AI are rather hard to come by. There are two reasons 
        for this. 
      <p>The first reason is that lots of people are either ignorant of or tendentiously 
        choose to ignore the underlying mathematical facts. These facts include 
        that some problems can be solved by computing machines, and others can't, 
        and that most can't. So whenever you ask whether a problem <i>P</i> can 
        be solved by a computing machine, where <i>P</i> is such that it isn't 
        known that there is an algorithm for solving it, honesty should imply 
        wait-and-see agnosticism.<a name=tex2html5 
href="#foot292"><sup>3</sup></a> 
      <p><br>
      <div align=center><a name=terminator.pov>&nbsp;</a><a name=293>&nbsp;</a> 
        <table width="50%">
          <caption><strong>Figure 2:</strong> POV of the Terminator in <em>T2</em></caption>
          <tbody> 
          <tr> 
            <td><!-- MATH: $\includegraphics[width=4in]{terminator2.ps}$ --><img 
      height=172 alt=\includegraphics[width=4in]{terminator2.ps} 
      src="3_files/img4.gif" width=460 align=bottom 
border=0></td>
          </tr>
          </tbody> 
        </table>
      </div>
      <br>
      <p>The second reason why realism in the face of questions like that which 
        drives the present investigation is in short supply is that we have lots 
        of silly prophets. For example, in the June 19, 2000 issue of <small>TIME</small> 
        magazine, devoted to ``The Future of Technology," we hear from author 
        and inventor Ray Kurzweil that nanobots (microscopic robots) will by 2030 
        be able to map out a synthetic duplicate of your brain after you swallow 
        (yes, swallow) a few of them. This duplicate will be instantiated in computer 
        hardware 10 million times faster than the sluggish, old-fashioned grey 
        stuff inside your cranium; the result will be an artificial intelligence 
        immeasurably more clever than you. Vernor Vinge, associate professor of 
        mathematics and computer science at San Diego State University, is another 
        example. Prophesying for the <em>Chronicle of Higher Education</em> (July 
        12, 2000; online edition), he gives us a more compressed timeline: by 
        his lights, on the strength of the trend that the speed of computer hardware 
        doubles every 18 months, computers will be more intelligent than all humans 
        at some point within 20 years. This point he calls ``The Singularity," 
        which ushers in post-humanity, an age in which humans are left in the 
        dust by machines that get exponentially smarter by the day (if not the 
        nanosecond). For a third example, consider Hans Moravec, who in his latest 
        book, <em>Robot: Mere Machine to Transcendent Mind</em>, informs us that 
        because hardware is getting faster at the rate Vinge cites, by 2040 ``fourth 
        generation" robots will exceed humans in all respects, from running companies 
        to writing novels. Such robots will evolve to such lofty cognitive heights 
        that we will stand to them as single-cell organisms stand to us today. 
        Many others in the field of Artificial Intelligence (AI) predict the same 
        sensational future unfolding on about the same rapid schedule. 
      <p>The gaming industry will not be this lucky; I'm sure of it. Today I'll 
        tell you, briefly, why. 
      <p> 
      <h1><a name=SECTION00030000000000000000>One Presupposition:&nbsp;There's 
        No Free Lunch</a> </h1>
      <p>Let make explicit one presupposition before we begin in earnest. I assume 
        that AI in general, along with the AI part of the gaming industry in particular, 
        have realized that non-logicist AI isn't magic. What do I mean? I mean 
        that people should be smart enough now to concede that no system is going 
        to automatically learn, through subsymbolic, numerical processing, how 
        to, say (and this is the challenge I'm going to focus on below), assemble 
        robust, autonomous characters in a game. Automated learning through artificial 
        neural networks and genetic algorithms are fine for certain applications, 
        but gone, I assume, are the days of wild optimism about the ability of 
        such learning techniques to automatically yield (to stick with this example) 
        full-blooded NPCs.<a 
name=tex2html6 
href="#foot294"><sup>4</sup></a> We really only have two overall 
        approaches to AI: one based on logic, and one based on subsymbolic processing. 
        This paper is written from the perspective of logic. If you think that 
        you have a way of solving the problems I'm talking about without using 
        logic, I wish you well. 
      <h1><a name=SECTION00040000000000000000>A List of Some Challenges</a> </h1>
      <p>Very well. So, why is it that building dramatically compelling interactive 
        digital entertainment (in the form, e.g., of games) is so difficult? There 
        are many reasons, among which fall the following. 
      <p> 
      <dl> 
        <dt><strong>C1:&nbsp;Formalizing Literary Themes.</strong> 
        <dd> 
          <p>If Dave Ferrucci and I are right, plotlines and so-called 3-dimensional 
            characters aren't enough for a computer to generate first-rate narrative: 
            you also need to instantiate immemorial themes -- betrayal (the one 
            we focus on in (<a 
  href="#brutus">Bringsjord &amp; Ferrucci, 2000</a>)), self-deception 
            (a theme we employ in <small>BRUTUS</small>), unrequited love, revenge, 
            and so on. If such themes are to be used by story-managing machines, 
            they must be represented; if they are to be represented and exploited, 
            they need to be <em>rigorously</em> represented and reasoned over. 
            Such rigorous representation and reasoning is very hard to come by. 
          </p>
        <dt><strong>C2:&nbsp;Story Mastery.</strong> 
        <dd> 
          <p>After interactive drama begins, things can devolve toward the uninteresting. 
            If ``hack-and-slash" is all that is sought from an interactive game, 
            then such devolution may be acceptable. But if genuine drama is desired, 
            then something or someone must ensure that what happens is dramatically 
            interesting. One possibility (with respect to a multi-player online 
            game) is to have a human or humans oversee the action as it unfolds, 
            and make changes that keep things ``on track." For obvious reasons, 
            in a rapidly progressing game with thousands of human players, this 
            is impracticable; the possibility of human oversight is purely conceptual. 
            So we must turn to computers to automate the process. But how? How 
            is the automation to work? I've assumed that if a program could be 
            built that writes compelling fiction, we might thereby have taken 
            significant steps toward a program that can serve as a story master. 
          </p>
        <dt><strong>C3:&nbsp;Building Robust, Autonomous Characters.</strong> 
        <dd> 
          <p>A <em>sine qua non</em> for compelling literature and drama is the 
            presence of robust, autonomous, and doxastically sophisticated<a 
  name="tex2html7" 
  href="#foot61"></sup>5</sup></a> characters. In short, such literature 
            and drama exploits the central properties of being a person. (In many 
            cases, great stories come to be remembered in terms of great characters.) 
            This presents a problem for interactive electronic entertainment: 
            how do we build an electronic character that has those attributes 
            that are central to personhood, and whose interaction with those humans 
            who enter the virtual worlds is thereby compelling? </p>
        <dt><strong>C4:&nbsp;Personalization.</strong> 
        <dd> 
          <p>If virtual characters are going to react intelligently to you as 
            user or gamer, they must, in some sense, <em>understand</em> you. 
            This problem is directly related to C3, because the characters must 
            have sophisticated beliefs about you and your beliefs, etc. </p>
        </dd>
        <dt>&nbsp;</dt>
      </dl>
      <p>In the remainder of this paper, I focus on C3, and moreover I focus within 
        this challenge on the specific problem of building autonomous characters. 
        The plan is as follows. I begin by reviewing the concept of an <b>intelligent 
        agent</b> in AI. I then explain the clash between this limited concept 
        and the kind of properties that are distinctive of personhood; one of 
        these properties is autonomy, or ``free will." In order to highlight the 
        problem of imparting autonomy to a virtual character, I turn to what I 
        have dubbed ``The Lovelace Test." I conclude with a disturbing argument 
        that seems to show that virtual characters, as intelligent agents, can't 
        be autonomous, because they would inevitably fail this test. I do intimate 
        my own reaction to this argument. 
      <h1><a name=SECTION00050000000000000000>Intelligent Agents</a> </h1>
      <p>As the century turns, all of AI has been to an astonishing degree unified 
        around the conception of an intelligent agent. The unification has in 
        large part come courtesy of a comprehensive textbook intended to cover 
        literally <em>all</em> of AI: Russell and Norvig's (<a 
href="#aima">1994</a>) <em>Artificial Intelligence:&nbsp;A Modern 
        Approach</em> (<em>AIMA</em>), the cover of which also displays the phrase 
        ``The Intelligent Agent Book." The overall, informal architecture for 
        an intelligent agent is shown in Figure <a 
href="#intelligent.agent">3</a>; this is taken directly from the 
        <em>AIMA</em> text. According to this architecture, agents take percepts 
        from the environment, process them in some way that prescribes actions, 
        perform these actions, take in new percepts, and continue in the cycle.<a name=tex2html14 
href="#foot295"><sup>6</sup></a> 
      <p><br>
      <div align=center><a name=intelligent.agent>&nbsp;</a><a name=296>&nbsp;</a> 
        <table width="50%">
          <caption><strong>Figure 3:</strong> The Architecture of an Intelligent 
          Agent</caption>
          <tbody> 
          <tr> 
            <td><!-- MATH: $\includegraphics[width=3in]{/home/58/brings/locker/fig02.01.ps}$ --><img 
      height=146 
      alt=\includegraphics[width=3in]{/home/58/brings/locker/fig02.01.ps} 
      src="6_files/img5.gif" width=344 align=bottom 
border=0></td>
          </tr>
          </tbody> 
        </table>
      </div>
      <br>
      <p>In <em>AIMA</em>, intelligent agents fall on a spectrum from least intelligent 
        to more intelligent to most intelligent. The least intelligent artificial 
        agent is a ``T<small>ABLE-</small>D<small>RIVEN-</small>A<small>GENT</small>," 
        the program (in pseudo-code) for which is shown in Figure <a 
href="#table.agent">4</a>. Suppose that we have a set of actions 
        each one of which is the utterance of a color name (``Green," ``Red," 
        etc.); and suppose that percepts are digital expressions of the color 
        of an object taken in by the sensor of a table-driven agent. Then given 
        Table <a 
href="#298">1</a> our simple intelligent agent, running the program 
        in Figure <a 
href="#table.agent">4</a>, will utter (through a voice synthesizer, 
        assume) ``Blue" if its sensor detects 100. Of course, this is a stunningly 
        dim agent. What are smarter ones like? 
      <p><br>
      <div align=center><a name=table.agent>&nbsp;</a><a name=297>&nbsp;</a> 
        <table width="50%">
          <caption><strong>Figure 4:</strong> The Least Intelligent Artificial 
          Agent</caption>
          <tbody> 
          <tr> 
            <td><!-- MATH: $\includegraphics[width=5in]
{/home/62/faheyj2/public_html/SB/SELPAP/ZOMBANIMALS/fig02.05.ps}$ --><img 
      height=141 
      alt="\includegraphics[width=5in]&#10;{/home/62/faheyj2/public_html/SB/SELPAP/ZOMBANIMALS/fig02.05.ps}" 
      src="6_files/img6.gif" width=448 align=bottom 
border=0></td>
          </tr>
          </tbody> 
        </table>
      </div>
      <br>
      <p><br>
      <div align=center> 
        <p><br>
        <div align=center></div>
        <div align=center> 
          <div align=center><a name=298>&nbsp;</a> 
            <table cellpadding=3 border=1>
              <caption><strong>Table 1:</strong> Lookup Table for T<small>ABLE-</small>D<small>RIVEN-</small>A<small>GENT</small></caption>
              <tbody> 
              <tr> 
                <td><font size=-1><b>Percept</b> </font></td>
                <td><font size=-1></font><font 
  size=-1><b>Action</b></font></td>
              </tr>
              <tr> 
                <td><font size=-1>001 </font></td>
                <td><font size=-1>``Red"</font></td>
              </tr>
              <tr> 
                <td><font size=-1>010 </font></td>
                <td><font size=-1>``Green"</font></td>
              </tr>
              <tr> 
                <td><font size=-1>100 </font></td>
                <td><font size=-1>``Blue"</font></td>
              </tr>
              <tr> 
                <td><font size=-1>011 </font></td>
                <td><font size=-1>``Yellow"</font></td>
              </tr>
              <tr> 
                <td><font size=-1>111 </font></td>
                <td><font size=-1>``Black"</font></td>
              </tr>
              <tr> 
                <td></td>
                <td></td>
              </tr>
              </tbody> 
            </table>
          </div>
        </div>
      </div>
      <br>
      <p><br>
      <div align=center><a name=kb.agent>&nbsp;</a><a name=299>&nbsp;</a> 
        <table width="50%">
          <caption><strong>Figure 5:</strong> Program for a Generic Knowledge-Based 
          Agent</caption>
          <tbody> 
          <tr> 
            <td><!-- MATH: $\includegraphics[width=5.5in]
{/home/62/faheyj2/public_html/SB/COURSES/INTAI/FIGS/fig06.01.ps}$ --><img 
      height=192 
      alt="\includegraphics[width=5.5in]&#10;{/home/62/faheyj2/public_html/SB/COURSES/INTAI/FIGS/fig06.01.ps}" 
      src="6_files/img7.gif" align=bottom 
border=0 width=338></td>
          </tr>
          </tbody> 
        </table>
      </div>
      <br>
      <p>In <em>AIMA</em> we reach artificial agents that might strike some as 
        rather smart when we reach the level of a ``knowledge-based" agent. The 
        program for such an agent is shown in Figure <a 
href="#kb.agent">5</a>. This program presupposes an agent that 
        has a knowledge-base (<em>KB</em>) in which what the agent knows is stored 
        in formulae in the propositional calculus, and the functions 
      <p> 
      <ul>
        <li>T<small>ELL</small>, which injects sentences (representing facts) 
          into <em>KB</em>; 
        <li>M<small>AKE-</small>P<small>ERCEPT-</small>S<small>ENTENCE</small>, 
          which generates a propositional calculus sentence from a percept and 
          the time <i>t</i> at which it is experienced; and 
        <li>M<small>AKE-</small>A<small>CTION-</small>S<small>ENTENCE</small>, 
          which generates a declarative fact (in, again, the propositional calculus) 
          expressing that an action has been taken at some time <i>t</i> </li>
      </ul>
      <p><br>
      <div align=center><a name=wumpus.world>&nbsp;</a><a name=300>&nbsp;</a> 
        <table width="50%">
          <caption><strong>Figure 6:</strong> A Typical Wumpus World</caption>
          <tbody> 
          <tr> 
            <td><!-- MATH: $\includegraphics[width=2.5in]
{/home/62/faheyj2/public_html/SB/COURSES/INTAI/FIGS/fig06.02.ps}$ --><img 
      height=276 
      alt="\includegraphics[width=2.5in]&#10;{/home/62/faheyj2/public_html/SB/COURSES/INTAI/FIGS/fig06.02.ps}" 
      src="6_files/img8.gif" width=285 align=bottom 
border=0></td>
          </tr>
          </tbody> 
        </table>
      </div>
      <br>
      <p>which give the agent the capacity to manipulate information in accordance 
        with the propositional calculus. (One step up from such an agent would 
        be a knowledge-based agent able to represent and reason over information 
        expressed in full first-order logic.) A colorful example of such an agent 
        is one clever enough to negotiate the so-called ``wumpus world." An example 
        of such a world is shown in Figure <a 
href="#wumpus.world">6</a>. The objective of the agent that finds 
        itself in this world is to find the gold and bring it back without getting 
        killed. As Figure <a 
href="#wumpus.world">6</a> indicates, pits are always surrounded 
        on three sides by breezes, the wumpus is always surrounded on three sides 
        by a stench, and the gold glitters in the square in which it's positioned. 
        The agent dies if it enters a square with a pit in it (interpreted as 
        falling into a pit) or a wumpus in it (interpreted as succumbing to an 
        attack by the wumpus). The percepts for the agent can be given in the 
        form of quadruples. For example, <br>
      <p></p>
      <div align=center><!-- MATH: \begin{displaymath}
\mbox{(Stench,Breeze,Glitter,None)}
\end{displaymath} --><img 
height=31 
alt=\begin{displaymath}\mbox{(Stench,Breeze,Glitter,None)}\end{displaymath} 
src="6_files/img9.gif" width=217> </div>
      <br clear=all>
      <p></p>
      <p>means that the agent, in the square in which it's located, perceives 
        a stench, a breeze, a glitter, and no scream. A scream occurs when the 
        agent shoots an arrow that kills the wumpus. There are a number of other 
        details involved, but this is enough to demonstrate how command over the 
        propositional calculus can give an agent a level of intelligence that 
        will allow it to succeed in the wumpus world. For the demonstration, let 
        <i>S</i><sub><i>i</i>,<i>j</i></sub> represent the fact that there is 
        a stench in column <i>i</i> row <i>j</i>, let <i>B</i><sub><i>i</i>,<i>j</i></sub> 
        denote that there is a breeze in column <i>i</i> row <i>j</i>, and let 
        <i>W</i><sub><i>i</i>,<i>j</i></sub> denote that there is a wumpus in 
        column <i>i</i> row <i>j</i>. Suppose now that an agent has the following 
        5 facts in its <em>KB</em>. </p>
      <p> 
      <dl compact> 
        <dt>1. 
        <dd><!-- MATH: $\neg S_{1,1} \wedge \neg S_{2,1} \wedge S_{1,2} \wedge \neg B_{1,1} \wedge 
B_{2,1} \wedge \neg B_{1,2}$ --><img 
  height=32 
  alt="$\neg S_{1,1} \wedge \neg S_{2,1} \wedge S_{1,2} \wedge \neg B_{1,1} \wedge&#10;B_{2,1} \wedge \neg B_{1,2}$" 
  src="6_files/img10.gif" width=328 align=middle border=0> 
        <dt>2. 
        <dd><!-- MATH: $\neg S_{1,1} \rightarrow (\neg W_{1,1} \wedge \neg W_{1,2} \wedge \neg
W_{2,1})$ --><img 
  height=34 
  alt="$\neg S_{1,1} \rightarrow (\neg W_{1,1} \wedge \neg W_{1,2} \wedge \neg&#10;W_{2,1}) $" 
  src="6_files/img11.gif" width=263 align=middle border=0> 
        <dt>3. 
        <dd><!-- MATH: $\neg S_{2,1} \rightarrow (\neg W_{1,1} \wedge \neg W_{2,1} \wedge \neg
W_{2,2} \wedge \neg W_{3,1})$ --><img 
  height=34 
  alt="$\neg S_{2,1} \rightarrow (\neg W_{1,1} \wedge \neg W_{2,1} \wedge \neg&#10;W_{2,2} \wedge \neg W_{3,1}) $" 
  src="6_files/img12.gif" width=328 align=middle border=0> 
        <dt>4. 
        <dd><!-- MATH: $\neg S_{1,2} \rightarrow (\neg W_{1,1} \wedge \neg W_{1,2} \wedge \neg
W_{2,2} \wedge \neg W_{1,3})$ --><img 
  height=34 
  alt="$\neg S_{1,2} \rightarrow (\neg W_{1,1} \wedge \neg W_{1,2} \wedge \neg&#10;W_{2,2} \wedge \neg W_{1,3}) $" 
  src="6_files/img13.gif" width=328 align=middle border=0> 
        <dt>5. 
        <dd><!-- MATH: $S_{1,2} \rightarrow (W_{1,3} \vee W_{1,2} \wedge
W_{2,2} \wedge W_{1,3})$ --><img 
  height=34 
  alt="$S_{1,2} \rightarrow (W_{1,3} \vee W_{1,2} \wedge&#10;W_{2,2} \wedge W_{1,3}) $" 
  src="6_files/img14.gif" width=270 align=middle border=0></dd>
      </dl>
      <p>Then in light of the fact that <br>
      <p></p>
      <div align=center><!-- MATH: \begin{displaymath}
\{1, \ldots, 5\} \vdash W_{1,3}
\end{displaymath} --><img 
height=32 
alt="\begin{displaymath}\{1, \ldots, 5\} \vdash W_{1,3}\end{displaymath}" 
src="6_files/img15.gif" width=126> </div>
      <br clear=all>
      <p></p>
      <p>in the propositional calculus,<a name=tex2html15 
href="#foot158"><sup>7</sup></a> the agent can come to know (= 
        come to include in its <em>KB</em>) that the wumpus is at location column 
        1 row 3 -- and this sort of knowledge should directly contribute to the 
        agent's success. </p>
      <p>In my lab, a number of students have built actual wumpus-world-winning 
        robots; for a picture of one toiling in this world see Figure <a 
href="#wumpus.robot">7</a>. 
      <p>Now I have no problem believing that the techniques and formalisms that 
        constitute the agent-based approach preached in <em>AIMA</em> are sufficient 
        to allow for the construction of characters that operate at the level 
        of animals. But when we reach the level of personhood, all bets, by my 
        lights, are off. 
      <p><br>
      <div align=center><a name=wumpus.robot>&nbsp;</a><a name=308>&nbsp;</a> 
        <table width="50%">
          <caption><strong><font face="Verdana, Arial, Helvetica, sans-serif" size="1">Figure:</font></strong><font face="Verdana, Arial, Helvetica, sans-serif" size="1"> 
          A Real-Life Wumpus-World-Winning Robot in the Minds <em>&amp;</em> Machines 
          Laboratory <em>(Observant readers may note that the wumpus here is represented 
          by a figurine upon which appears the (modified) face of the Director 
          of the M &amp;M Lab: Bringsjord.)</em></font></caption>
          <tbody> 
          <tr> 
            <td><!-- MATH: $\includegraphics[width=3in]{ww1.ps}$ --><img height=214 
      alt=\includegraphics[width=3in]{ww1.ps} src="6_files/img16.gif" width=345 
      align=bottom border=0></td>
          </tr>
          </tbody> 
        </table>
      </div>
      <h1><a name=SECTION00060000000000000000>The Roadblock:&nbsp;Personhood</a> 
      </h1>
      <p>Why is it that intelligent agent techniques will allow us to build virtual 
        rats, parrot, and chimps, but fail when we attempt to build virtual persons? 
        They will fail because intelligent agent architectures, formalisms, tools, 
        and so on are impotent in the face of the properties that distinguish 
        persons. What are these properties? Many philosophers have taken up the 
        challenge of answering this question, but for present purposes it suffices 
        to call upon an account of personhood offered in (<a 
href="#abortion.dialogue">Bringsjord, 1997</a>); in fact, it suffices 
        to list here only five of the properties offered in that account, viz.,<a name=tex2html16 
href="#foot302"><sup>8</sup></a> 
      <p> 
      <dl compact> 
        <dt>1. ability to communicate in a language 
        <dt>2. autonomy (``free will") 
        <dt>3. creativity 
        <dt>4. phenomenal consciousness 
        <dt>5. robust abstract reasoning (e.g., ability to create conceptual schemes, 
          and to switch from one to another) 
      </dl>
      <p>For the sake of argument I'm prepared to follow Turing and hold that 
        AI will engineer not only the communicative powers of a parrot and a chimp, 
        but also the linguistic powers of a human person. (This concession requires 
        considerable optimism: The current state-of-the art in AI is unable to 
        create a device with the linguistic capacity of a toddler.) However, it's 
        exceedingly hard to see how each of the four remaining properties can 
        be reduced to the machinery of the intelligent agent paradigm in AI. Intelligent 
        agents don't seem to originate anything; they seem to do just what they 
        have been designed to do. And so it's hard to see how they can originate 
        decisions and actions (``free will") or artifacts (creativity). At least 
        at present, it's hard to see how phenomenal consciousness can be captured 
        in any third-person scheme whatever (and as many readers will know, a 
        number of philosophers -- Nagel, e.g.&nbsp;-- have argued that such consciousness 
        can never be captured in such a scheme), let alone in something as austere 
        as what AI engineers work with. And those in AI who seek to model abstract 
        reasoning know well that we have only begun to show how sophisticated 
        abstract reasoning can be cast in well-understood computable logics. For 
        all we know at present, it may be that some of this reasoning is beyond 
        the reach of computation. Certainly such reasoning cannot be cashed out 
        in the vocabulary of <em>AIMA</em>, which stays firmly within extensional 
        first-order logic. 
      <p>But let's focus, as I said I would, on the issue of autonomous intelligent 
        agents. I believe I have a way of sharpening the challenge that this issue 
        presents to those who aspire to create dramatically compelling interactive 
        electronic entertainment. This way involves subjecting would-be autonomous 
        virtual characters to a form of the Lovelace Test. But first, I have to 
        introduce the test. 
      <h1><a name=SECTION00070000000000000000>The Lovelace Test</a> </h1>
      <p>As you probably know, Turing predicted in his famous ``Computing Machinery 
        and Intelligence" (<a 
href="#turing.tt">1964</a>) that by the turn of the century computers 
        would be so smart that when talking to them from a distance (via email, 
        if you will) we would not be able to tell them from humans: they would 
        be able to pass what is now known as the Turing Test (TT). Well, New Year's 
        Eve of 1999 has come and gone, all the celebratory pyrotechnics have died, 
        and the fact is: AI hasn't managed to produce a computer with the conversational 
        punch of a toddler. 
      <p>But the really depressing thing is that though progress toward Turing's 
        dream is being made, it's coming only on the strength of clever but shallow 
        trickery. For example, the human creators of artificial agents that compete 
        in present-day versions of TT know all too well that they have merely 
        tried to <em>fool</em> those people who interact with their agents into 
        believing that these agents really have minds. In such scenarios it's 
        really the human creators against the human judges; the intervening computation 
        is in many ways simply along for the ride. 
      <p>It seems to me that a better test is one that insists on a certain restrictive 
        epistemic relation between a an artificial agent <i>A</i>, its output 
        <i>o</i>, and the human architect <i>H</i> of <i>S</i> -- a relation which, 
        roughly speaking, obtains when <i>H</i> cannot account for how <i>A</i> 
        produced <i>o</i>. I call this test the ``Lovelace Test" in honor of Lady 
        Lovelace, who believed that only when computers <em>originate</em> things 
        should they be believed to have minds. 
      <p> 
      <h2><a name=SECTION00071000000000000000>The Lovelace Test in More Detail</a> 
      </h2>
      <p>To begin to see how LT works, we start with a scenario that is close 
        to home for Bringsjord and Ferrucci, given their sustained efforts to 
        build story generation agents: Assume that Jones, a human AInik, attempts 
        to build an artificial computational agent <i>A</i> that doesn't engage 
        in conversation, but rather creates stories -- creates in the Lovelacean 
        sense that this system <em>originates</em> stories. Assume that Jones 
        activates <i>A</i> and that a stunningly belletristic story <i>o</i> is 
        produced. We claim that if Jones cannot explain how <i>o</i> was gemerated 
        by <i>A</i>, <em>and</em> if Jones has no reason whatever to believe that 
        <i>A</i> succeeded on the strength of a fluke hardware error, etc.&nbsp;(which 
        entails that <i>A</i> can produce other equally impressive stories), then 
        <i>A</i> should at least provisionally be regarded genuinely creative. 
        An artificial computational agent passes LT if and only if it stands to 
        its creator as <i>A</i> stands to Jones. 
      <p>LT relies on the special epistemic relationship that exists between Jones 
        and <i>A</i>. But `Jones,' like `<i>A</i>,' is of course just an uninformative 
        variable standing in for any human system designer. This yields the following 
        rough-and-ready definition. 
      <p> 
      <dl> 
        <dt><strong>Def<sub><i>LT</i></sub> 1</strong> 
        <dd>Artificial agent <i>A</i>, designed by <i>H</i>, passes LT if and 
          only if 
          <dl> 
            <dt><strong>1</strong> <i>A</i> outputs <i>o</i>; 
            <dt><strong>2</strong> <i>A</i>'s outputting <i>o</i> is not the result 
              of a fluke hardware error, but rather the result of processes <i>A</i> 
              can repeat; 
            <dt><strong>3</strong> <i>H</i> (or someone who knows what <i>H</i> 
              knows, and has <i>H</i>'s resources<a name=tex2html17 
    href="#foot187"><sup>9</sup></a>) cannot explain how <i>A</i> 
              produced <i>o</i>. 
          </dl>
        </dd>
      </dl>
      <p>Notice that LT is actually what might be called a <em>meta</em>-test. 
        The idea is that this scheme can be deployed for any partcular domain. 
        If conversation is the kind of behavior wanted, then merely stipulate 
        that <i>o</i> is an English sentence (or sequence of such sentences) in 
        the context of a converation (as in, of course, TT). If the production 
        of a mathematical proof with respect to a given conjecture is what's desired, 
        then we merely set <i>o</i> to a proof. In light of this, we can focus 
        LT on the particular kind of interaction appropriate for the digital entertainment 
        involved. 
      <p>Obvious questions arise at this point. Three are: 
      <p>&nbsp; 
      <p> </p>
      <p><strong>Q1</strong> What resources and knowledge does <i>H</i> have at 
        his or her disposal?</p>
      <p><strong>Q2</strong> What sort of thing would count as a successful explanation? 
      </p>
      <p><strong>Q3</strong> How long does <i>H</i> have to cook up the explanation? 
      </p>
      <p>&nbsp; </p>
      <p>The answer to the third question is easy: <i>H</i> can have as long as 
        he or she likes, within reason. The proffered explanation doesn't have 
        to come immediately: <i>H</i> can take a month, months, even a year or 
        two. Anything longer than a couple of years strikes us as perhaps unreasonable. 
        We realize that these temporal parameters aren't exactly precise, but 
        then again we should not be held to standards higher than those pressed 
        against Turing and those who promote his test and variants thereof.<a name=tex2html18 
href="#foot303"><sup>10</sup></a> The general point, obviously, 
        is that <i>H</i> should have more than ample time to sort things out. 
      <p>But what about Q1 and Q2? The answer to Q1 is that <i>H</i> is assumed 
        to have at her disposal knowledge of the architecture of the agent in 
        question, knowledge of the KB of the agent, knowledge of how the main 
        functions in the agent are implemented (e.g., how T<small>ELL</small> 
        and A<small>SK</small> are implemented), and so on (recall the summary 
        of intelligent agents above). <i>H</i> is also assumed to have resources 
        sufficient to pin down these elements, to ``freeze" them and inspect them, 
        and so on. I confess that this isn't exactly precise. To clarify things, 
        I offer an example. This example is also designed to provide an answer 
        to Q2. 
      <p>To fix the context for the example, suppose that the output from our 
        artificial agent <i>A</i>' is a resolution-based proof which settles a 
        problem which human mathematicians and logicians have grappled unsuccessfully 
        with for decades. This problem, suppose, is to determine whether or not 
        some formula <img 
height=32 alt=$\phi$ src="9_files/img18.gif" width=15 align=middle border=0> can 
        be derived from some (consistent) axiom set <img height=15 alt=$\Gamma$ 
src="9_files/img19.gif" width=16 align=bottom border=0>. Imagine that after many 
        years of fruitless deliberation, a human <i>H</i>' encodes <img height=15 
alt=$\Gamma$ src="9_files/img19.gif" width=16 align=bottom border=0> and <img 
height=32 alt="$\neg \phi$" src="9_files/img20.gif" width=27 align=middle 
border=0> and gives both to <small>OTTER</small> (a well-known theorem prover; 
        it's discussed in (<a 
href="#brutus">Bringsjord &amp; Ferrucci, 2000</a>)), and <small>OTTER</small> 
        produces a proof showing that this encoding is inconsistent, which establishes 
        <!-- MATH: $\Gamma
\vdash
\phi$ --><img height=32 
alt="$\Gamma&#10;\vdash&#10;\phi$" src="9_files/img21.gif" width=46 align=middle 
border=0>, and leads to an explosion of commentary in the media about ``brilliant" 
        and ``creative" machines, and so on.<a name=tex2html19 
href="#foot304"><sup>11</sup></a> In this case, <i>A</i>' doesn't 
        pass LT. This is true because <i>H</i>, knowing the KB, architecture, 
        and central functions of <i>A</i>' will be able to give a perfect explanation 
        for the behavior in question. I routinely give explanations of this sort. 
        The KB is simply the encoding of <!-- MATH: $\Gamma \cup
\{\phi\}$ --><img height=34 
alt="$\Gamma \cup&#10;\{\phi\}$" src="9_files/img22.gif" width=63 align=middle 
border=0>, the architecture consists in the search algorithms used by <small>OTTER</small>, 
        and the main functions consist in the rules of inference used in a resolution-based 
        theorem prover. 
      <p>&nbsp; 
      <p>Here, now, given the foregoing, is a better definition: 
      <p> 
      <dl> 
        <dt><strong>Def<sub><i>LT</i></sub> 2</strong> 
        <dd> 
          <p>Artificial agent <i>A</i>, designed by <i>H</i>, passes LT if and 
            only if </p>
        </dd>
        <dd> 
          <dl> 
            <dt><strong>1</strong> <i>A</i> outputs <i>o</i>; 
            <dt><strong>2</strong> <i>A</i>'s outputting <i>o</i> is not the result 
              of a fluke hardware error, but rather the result of processes <i>A</i> 
              can repeat; 
            <dt><strong>3</strong> <i>H</i> (or someone who knows what <i>H</i> 
              knows, and has <i>H</i>'s resources) cannot explain how <i>A</i> 
              produced <i>o</i> by appeal to <i>A</i>'s architecture, knowledge-base, 
              and core functions. 
          </dl>
        </dd>
      </dl>
      <!--End of Navigation Panel--> 
      <h2><a name=SECTION00072000000000000000>How do Today's Systems Fare in the 
        Lovelace Test?</a> </h2>
      <p>Today's systems, even those designed to either be, or seem to be, autonomous, 
        fail LT. These designers can imagine themselves generating the output 
        in question by merely manipulating symbols in accordance with the knowledge 
        bases, algorithms, and code in question. We give an example of this kind 
        of failure, an example that falls rather close to home for me. <br>
        <!--End of Navigation Panel--> 
      <h2><a name=SECTION00073000000000000000>Why <small>BRUTUS</small> Fails 
        the Lovelace Test</a> </h2>
      <p>The <small>BRUTUS</small> system is designed to appear to be literarily 
        creative to <em>others</em>. To put the point in the spirit of the Turing 
        Test, <small>BRUTUS</small> reflects a multi-year attempt to build a system 
        able to play the short short story game, or S<sup>3</sup>G for short (<a 
href="#brings.mitr">Bringsjord, 1998</a>). (See Figure <a 
href="#s3g">8</a> for a picture of S<sup>3</sup>G.) 
      <p><br>
      <div align=center><a name=s3g>&nbsp;</a><a name=306>&nbsp;</a> 
        <table width="50%">
          <caption><strong>Figure 8:</strong> The Short Short Story Game, or S<sup>3</sup>G 
          for Short.</caption>
          <tbody> 
          <tr> 
            <td><!-- MATH: $\includegraphics[width=2in]{s3g.xfig.ps}$ --><img 
      height=292 alt=\includegraphics[width=2in]{s3g.xfig.ps} 
      src="11_files/img23.gif" width=212 align=bottom 
border=0></td>
          </tr>
          </tbody> 
        </table>
      </div>
      <br>
      <p>The idea behind S<sup>3</sup>G is simple. A human and a computer compete 
        against each other. Both receive one relatively simple sentence, say: 
        ``As Gregor Samsa awoke one morning from uneasy dreams he found himself 
        transformed in his bed into a gigantic insect." (Kafka <a 
href="#kafka.metamorphosis">1948</a>, p.&nbsp;67) Both mind and 
        machine must now fashion a short short story (about 500 words) designed 
        to be truly interesting; the more literary virtue, the better. The goal 
        in building <small>BRUTUS</small>, then, is to build an artificial author 
        able to compete with first-rate human authors in S<sup>3</sup>G, much 
        as Deep Blue went head to head with Kasparov. 
      <p>How does <small>BRUTUS</small> fare? Relative to the goal of passing 
        S<sup>3</sup>G, not very well. On the other hand, <small>BRUTUS</small> 
        can ``author" some rather interesting stories (<a 
href="#brutus">Bringsjord &amp; Ferrucci, 2000</a>). Note that 
        we have placed the term `author' in scare quotes. Why? The reason is plain 
        and simple, and takes us back to Lady Lovelace's objection: <small>BRUTUS</small> 
        doesn't <em>originate</em> stories. He is capable of generating it because 
        two humans, Bringsjord and Ferrucci, spent years figuring out how to formalize 
        a generative capacity sufficient to produce this and other stories, and 
        they then are able to implement part of this formalization so as to have 
        a computer produce such prose. This method is known as <em>reverse engineering</em>. 
        Obviously, with <small>BRUTUS</small> set to <i>A</i> and Bringsjord and 
        Ferrucci set to <i>H</i> in the definition of LT, the result is that <small>BRUTUS</small> 
        fails this test. 
      <p>Let's now give you, briefly, a specific example to make this failure 
        transparent. <small>BRUTUS</small> is programmed to produce stories that, 
        are, at least to some degree, bizarre. The reason for this is that reader 
        response research tells us that readers are engaged by bizarre material. 
        Now, in <small>BRUTUS</small>, to express the bizarre, modifiers are linked 
        with objects in frames named <tt>bizzaro_modifiers</tt>. Consider the 
        following instance describing the <tt>bizzaro</tt> modifier <tt>bleeding</tt>. 
      <p> 
      <div align=center></div>
      <!-- MATH: $\fbox{\fbox{\begin{minipage}[b]{10.7cm}
\par
\noindent {\tt instance bleeding is a bizzaro\_modifier}
\par
\hspace{.25in} {\tt objects are \{sun, plants, clothes, tombs, eyes\}.}
\end{minipage}}}$ --><img 
height=58 
alt="\fbox{\fbox{\begin{minipage}[b]{10.7cm}&#10;\par&#10;\noindent {\tt instance bleeding is...&#10;....25in} {\tt objects are \{sun, plants, clothes, tombs, eyes\}.}&#10;\end{minipage}}}" 
src="11_files/img24.gif" width=507 align=bottom border=0> 
      <div align=center></div>
      <p>What Bringsjord and Ferrucci call <b>literary augmented grammars</b>, 
        or just a LAGs, may be augmented with constraints to stimulate bizarre 
        images in the mind of the reader. The following LAG for action analogies, 
      <p> 
      <ul>
        <li><font size="2"><tt>BizarreActionAnalogy</tt> <img height=16 alt=$\rightarrow$ 
  src="11_files/img25.gif" width=22 align=bottom border=0> <tt>NP VP like ANP</tt> 
          </font> 
        <li><font size="2"><tt>NP</tt> <img height=16 alt=$\rightarrow$ src="11_files/img25.gif" 
  width=22 align=bottom border=0> <tt>noun_phrase</tt> </font> 
        <li><font size="2"><tt>ANP</tt> <img height=16 alt=$\rightarrow$ src="11_files/img25.gif" 
  width=22 align=bottom border=0> <tt>modifier (isa bizzaro_modifier) noun (isa 
          analog of NP)</tt> </font></li>
      </ul>
      <p>in conjunction with <tt>bizzaro_modifiers</tt>, can be used by <small>BRUTUS</small> 
        to generate the following sentence. 
      <p> 
      <blockquote> 
        <p><em>Hart's eyes were like big bleeding suns.</em></p>
      </blockquote>
      <p>Sentences like this in output from <small>BRUTUS</small> are therefore 
        a function of work carried out by (in this case) Ferrucci. Such sentences 
        do not result from <small>BRUTUS</small> thinking on its own. 
      <h1><a name=SECTION00080000000000000000>The Conclusing Argument</a> </h1>
      <p>What does the Lovelace Test buy us? What role does it play in connection 
        with challenge C3? The overall idea is this. A truly autonomous virtual 
        character is an intelligent agent that has those attributes constitutive 
        of personhood, attributes that include autonomy. Operationalized, this 
        means that truly autonomous virtual characters would pass LT. But such 
        agents <em>can't</em> pass LT. Put in the form of an argument, and tied 
        to the question that gives this paper its title, we have: 
      <p>&nbsp; 
      <p> 
      <blockquote></blockquote>
      <div align=center> 
        <blockquote> 
          <h3><font size=-1>The Argument That Worries Me</font></h3>
        </blockquote>
      </div>
      <dl compact> 
        <dt>1. Dramatically compelling interactive digital entertainment requires 
          the presence in such entertainment of virtual persons, and therefore 
          requires the presence of autonomous virtual characters. 
        <dt>2. Autonomous virtual characters would pass the Lovelace Test. 
        <dt>3. Autonomous virtual characters would be intelligent agents, in the 
          technical sense of ``intelligent agents" in use in AI (specifically 
          in <em>AIMA</em>). 
        <dt>4. Intelligent agents fail the Lovelace Test. 
        <dt>5. Dramatically compelling interactive digital entertainment isn't 
          possible. 
      </dl>
      <blockquote></blockquote>
      <p>What should the response be to this argument be? Perhaps you'll need 
        to think about what your own reaction should be; the point of this paper 
        is only to place the argument before you. Clearly, the argument is formally 
        valid, that is, the logic is correct: 5.&nbsp;does follow from the premises. 
        So to escape the argument, at least one of the premises must be rejected. 
        My suspicion is that premise 1.&nbsp; is false, but that what's true is 
        a relative, viz., 
      <p> 
      <dl> 
        <dt><strong>1'.</strong> Dramatically compelling interactive digital entertainment 
          requires the presence in such entertainment of <em>seemingly</em> autonomous 
          virtual characters. 
      </dl>
      <p>If this is right, those who design and build digital entertainment need, 
        at bottom, to figure out ingenious ways of fooling players into believing 
        that virtual characters are, in general, persons (and hence, among other 
        things, autonomous). The job description for those intent on building 
        dramatically compelling interative digital entertainment thus calls for 
        those who can figure out the stimuli that impel gamers to believe they 
        are interacting with virtual people, and then engineer a system to produce 
        this stimuli in a principled way. This job description is decidely <em>not</em> 
        filled by those in game development who have mastered the so-called present-day 
        ``art of character design," which is nicely summarized, e.g., in (<a 
href="#building.character">Gard, 2000</a>). Why this is so, and 
        what, as a practical engineering matter, needs to be done to extend present-day 
        techniques -- well, this will need to wait for another day. 
      <p>&nbsp; 
      <p align="center"><font face="Verdana, Arial, Helvetica, sans-serif"><a class="note" href="#top">[To 
        the top of the page]</a></font> 
      <h2>Notes</h2>
      <p> 
      <blockquote> 
        <h6><a name="presented"></a>* This article was first presented as a paper 
          at the 2001 <a href="http://diac.it-c.dk/cgdt">Computer Games &amp; 
          Digital Textualities</a> conference in Copenhagen.</h6>
      </blockquote>
      <dd> 
      <dt><a name=foot13>.. games)?</a><a name=foot13 
  href="#tex2html1"><sup>1</sup></a> 
      <dd> 
        <h6>I'm indebted to Dave Ferrucci, Devin Croak, and Marc Destefano.</h6>
      <dd>
        
      <dt><a name=foot34>... games.</a><a name=foot34 
  href="#tex2html3"><sup>2</sup></a> 
      <dd> 
          <h6>Playing soccer against someone with Playstation 2 running to a large 
            plasma display is, for me, fun. Perhaps you, on the other hand, prefer 
            a current online multi-player game or two. </h6>
        <dd> 
          <pre></pre>
        
      <dt><a name=foot292>... agnosticism.</a><a name=foot292 
  href="#tex2html5"><sup>3</sup></a> 
      <dd> 
          
        <h6>I have recently argued that deciding whether some story is interesting 
          is a computationally unsolvable problem. See ``Chapter 5:&nbsp;The Narrative-Based 
          Refutation of Church's Thesis" in (<a 
  href="#brutus">Bringsjord &amp; Ferrucci, 2000</a>). </h6>
        <dd> 
          <pre></pre>
        
      <dt><a name=foot294>... NPCs.</a><a name=foot294 
  href="#tex2html6"><sup>4</sup></a> 
      <dd> 
          
        <h6>The connectionist-logicist clash in AI is discussed in: (<a 
  name=tex2html163 
  href="#lai">Bringsjord &amp; Ferrucci, 1998</a>,<a name=tex2html164 
  href="#brutus">2000</a>; <a name=tex2html165 
  href="#jetai.clash">Bringsjord, 1991</a>). The last of these 
          publications constitutes an introduction to logicist AI. </h6>
        <dd> 
          <pre></pre>
        
      <dt><a name=foot61>... sophisticated</a><a name=foot61 
  href="#tex2html7"><sup>5</sup></a> 
      <dd> 
          <h6>A character is doxastically sophisticated if it can reason over 
            its beliefs about the beliefs other characters and human users have 
            about beliefs, etc. </h6>
        <dd> 
          <pre></pre>
        
      <dt><a name=foot295>... cycle.</a><a name=foot295 
  href="#tex2html14"><sup>6</sup></a> 
      <dd> 
          
        <h6>The cycle here is strikingly similar to the overall architecture of 
          cognition described by <a 
  href="#pollock.cc">Pollock (1995)</a>. </h6>
        <dd> 
          <pre></pre>
        
      <dt><a name=foot158>... calculus,</a><a name=foot158 
  href="#tex2html15"><sup>7</sup></a> 
      <dd> 
          <h6>The proof is left to sedulous readers. </h6>
        <dd> 
          <pre></pre>
        
      <dt><a name=foot302>... viz.,</a><a name=foot302 
  href="#tex2html16"><sup>8</sup></a> 
      <dd> 
          <h6>The account is streamlined in the interests of space. For example, 
            because people sleep (and because they can be hypnotized, etc.), a 
            person would be a creature with the <em>capacity</em> to have properties 
            like those listed here. </h6>
        <dd> 
          <pre></pre>
        
      <dt><a name=foot187>... resources</a><a name=foot187 
  href="#tex2html17"><sup>9</sup></a> 
      <dd> 
          <h6>For example, the substitute for <i>H</i> might be a scientist who 
            watched and assimilated what the designers and builders of <i>A</i> 
            did every step along the way. </h6>
        <dd> 
          <pre></pre>
        
      <dt><a name=foot303>... thereof.</a><a name=foot303 
  href="#tex2html18"><sup>10</sup></a> 
      <dd> 
          
        <h6>In (<a 
  href="#androids">Bringsjord, 1995</a>), Bringsjord refutes propositions 
          associated with TT by assuming for the sake of argument that some reasonable 
          parameters <img height=15 alt=$\pi$ 
  src="Footnotes_files/img17.gif" width=15 align=bottom border=0> have been established 
          for this test. But Turing didn't specify <img height=15 alt=$\pi$ 
  src="Footnotes_files/img17.gif" width=15 align=bottom border=0>, and neither 
          have his present-day defenders. </h6>
        <dd> 
          <pre></pre>
        
      <dt><a name=foot304>... on.</a><a name=foot304 
  href="#tex2html19"><sup>11</sup></a> 
      <dd> 
          
        <h6>For a ``real life" counterpart, we have <small>OTTER</small>'s settling 
          the Robbins Problem, presented as an open question in (<a 
  href="#otter.notebook">Wos, 1996</a>). </h6>
        </dd>
      
      <h2><a name=SECTIONREF>Bibliography</a> </h2>
      <dl compact> 
        <dt><a name=jetai.clash><strong>Bringsjord, S. (1991),</strong></a> 
        <dd> 
          <h6>`Is the connectionist-logicist clash one of ai's wonderful red herrings?', 
            <em>Journal of Experimental &amp; Theoretical AI</em> <b>3.4</b>,&nbsp;319-349. 
          </h6>
        <dd> 
          <h6></h6>
        <dt><a name=androids><strong>Bringsjord, S. (1995),</strong></a> 
        <dd> 
          <h6>Could, how could we tell if, and why should-androids have inner 
            lives?, <em>in</em> K.&nbsp;Ford, C.&nbsp;Glymour &amp; P.&nbsp;Hayes, 
            eds, `Android Epistemology', MIT Press, Cambridge, MA, pp.&nbsp;93-122. 
          </h6>
          <h6></h6>
        <dt><a name=abortion.dialogue><strong>Bringsjord, S. (1997),</strong></a> 
        <dd> 
          <h6><em>Abortion: A Dialogue</em>, Hackett, Indianapolis, IN. </h6>
          <h6></h6>
        <dt><a name=brings.mitr><strong>Bringsjord, S. (1998),</strong></a> 
        <dd> 
          <h6>`Chess is too easy', <em>Technology Review</em> <b>101</b>(2),&nbsp;23-28. 
          </h6>
          <h6></h6>
        <dt><a name=lai><strong>Bringsjord, S. &amp; Ferrucci, D. ( 1998),</strong></a> 
        <dd> 
          <h6>`Logic and artificial intelligence: Divorced, still married, separated...?', 
            <em>Minds and Machines</em> <b>8</b>,&nbsp;273-308. </h6>
          <h6></h6>
        <dt><a name=brutus><strong>Bringsjord, S. &amp; Ferrucci, D. ( 2000),</strong></a> 
        <dd> 
          <h6><em>Artificial Intelligence and Literary Creativity: Inside the 
            Mind of Brutus, a Storytelling Machine</em>, Lawrence Erlbaum, Mahwah, 
            NJ. </h6>
          <h6></h6>
        <dt><a name=building.character><strong>Gard, T. (2000),</strong></a> 
        <dd> 
          <h6>`Building character', <em>Game Developer Magazine</em> <b>7.5</b>,&nbsp;28-37. 
          </h6>
          <h6></h6>
        <dt><a name=kafka.metamorphosis><strong>Kafka, F. (1948),</strong></a> 
        <dd> 
          <h6>The metamorphosis, <em>in</em> F.&nbsp;Kafka, t.&nbsp;W. Muir &amp; 
            E.&nbsp;Muir, eds, `The Penal Colony', Schocken Books, New York, NY. 
          </h6>
          <h6></h6>
        <dt><a name=pollock.cc><strong>Pollock, J. (1995),</strong></a> 
        <dd> 
          <h6><em>Cognitive Carpentry: A Blueprint for How to Build a Person</em>, 
            MIT Press, Cambridge, MA. </h6>
          <h6></h6>
        <dt><a name=aima><strong>Russell, S. &amp; Norvig, P. (1994),</strong></a> 
        <dd> 
          <h6><em>Artificial Intelligence: A Modern Approach</em>, Prentice Hall, 
            Saddle River, NJ. </h6>
          <h6></h6>
        <dt><a name=turing.tt><strong>Turing, A. (1964),</strong></a> 
        <dd> 
          <h6>Computing machinery and intelligence, <em>in</em> A.&nbsp;R. Anderson, 
            ed., `Minds and Machines', Prentice-Hall, Englewood Cliffs, NJ, pp.&nbsp;4-30. 
          </h6>
          <h6></h6>
        <dt><a name=otter.notebook><strong>Wos, L. (1996),</strong></a> 
        <dd> 
          <h6><em>The Automation of Reasoning: An Experimenter's Notebook with 
            <small>OTTER</small> Tutorial</em>, Academic Press, San Diego, CA. 
          </h6>
          <h6></h6>
        </dd>
      </dl>
      <h6> </h6>
      <h6 align="center"><font face="Verdana, Arial, Helvetica, sans-serif"><a class="note" href="#top">[To 
        the top of the page]</a></font> </h6>
      <!-- #EndEditable --></td>
    <td width="143" height="*" align="left"><!-- #BeginEditable "right" --><!-- #EndEditable -->&nbsp; 
      <p>&nbsp; 
    </td>
  </tr>
</table>

</body>
<!-- #EndTemplate --></html>
